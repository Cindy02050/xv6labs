diff --git a/kernel/bio.c b/kernel/bio.c
index 60d91a6..7aa31a3 100644
--- a/kernel/bio.c
+++ b/kernel/bio.c
@@ -23,32 +23,37 @@
 #include "fs.h"
 #include "buf.h"
 
+#define NBUFMAP_BUCKET 13
+#define BUFMAP_HASH(dev, blockno) ((((dev)<<27)|(blockno))%NBUFMAP_BUCKET)
+
 struct {
-  struct spinlock lock;
   struct buf buf[NBUF];
+  struct spinlock eviction_locks[NBUFMAP_BUCKET];
 
-  // Linked list of all buffers, through prev/next.
-  // Sorted by how recently the buffer was used.
-  // head.next is most recent, head.prev is least.
-  struct buf head;
+  // Hash map: dev and blockno to buf
+  struct buf bufmap[NBUFMAP_BUCKET];
+  struct spinlock bufmap_locks[NBUFMAP_BUCKET];
 } bcache;
 
 void
 binit(void)
 {
-  struct buf *b;
-
-  initlock(&bcache.lock, "bcache");
+  // Initialize bufmap
+  for(int i=0;i<NBUFMAP_BUCKET;i++) {
+    initlock(&bcache.eviction_locks[i], "bcache_eviction");
+    initlock(&bcache.bufmap_locks[i], "bcache_bufmap");
+    bcache.bufmap[i].next = 0;
+  }
 
-  // Create linked list of buffers
-  bcache.head.prev = &bcache.head;
-  bcache.head.next = &bcache.head;
-  for(b = bcache.buf; b < bcache.buf+NBUF; b++){
-    b->next = bcache.head.next;
-    b->prev = &bcache.head;
+  // Initialize buffers
+  for(int i=0;i<NBUF;i++){
+    struct buf *b = &bcache.buf[i];
     initsleeplock(&b->lock, "buffer");
-    bcache.head.next->prev = b;
-    bcache.head.next = b;
+    b->lastuse = 0;
+    b->refcnt = 0;
+    // put all the buffers into bufmap[0]
+    b->next = bcache.bufmap[0].next;
+    bcache.bufmap[0].next = b;
   }
 }
 
@@ -60,32 +65,114 @@ bget(uint dev, uint blockno)
 {
   struct buf *b;
 
-  acquire(&bcache.lock);
+  uint key = BUFMAP_HASH(dev, blockno);
+
+  // printf("dev: %d, blockno: %d, locked: %d\n", dev, blockno, bcache.bufmap_locks[key].locked);
+  
+  acquire(&bcache.bufmap_locks[key]);
 
   // Is the block already cached?
-  for(b = bcache.head.next; b != &bcache.head; b = b->next){
+  for(b = bcache.bufmap[key].next; b; b = b->next){
     if(b->dev == dev && b->blockno == blockno){
       b->refcnt++;
-      release(&bcache.lock);
+      release(&bcache.bufmap_locks[key]);
       acquiresleep(&b->lock);
       return b;
     }
   }
 
   // Not cached.
-  // Recycle the least recently used (LRU) unused buffer.
-  for(b = bcache.head.prev; b != &bcache.head; b = b->prev){
-    if(b->refcnt == 0) {
-      b->dev = dev;
-      b->blockno = blockno;
-      b->valid = 0;
-      b->refcnt = 1;
-      release(&bcache.lock);
+
+  // to get a suitable block to reuse, we need to search for one in all the buckets,
+  // which means acquiring their bucket locks.
+  // but it's not safe to try to acquire every single bucket lock while holding one.
+  // it can easily lead to circular wait, which produces deadlock.
+
+  release(&bcache.bufmap_locks[key]);
+  // we need to release our bucket lock so that iterating through all the buckets won't
+  // lead to circular wait and deadlock. however, as a side effect of releasing our bucket
+  // lock, other cpus might request the same blockno at the same time and the cache buf for  
+  // blockno might be created multiple times in the worst case. since multiple concurrent
+  // bget requests might pass the "Is the block already cached?" test and start the 
+  // eviction & reuse process multiple times for the same blockno.
+  //
+  // so, after acquiring eviction_locks[key], we check "whether cache for blockno is present"
+  // once more, to be sure that we don't create duplicate cache bufs.
+  //
+  // thanks @ttzytt for pointing out that eviction_lock really just need to be per-bucket instead
+  // of globally shared.
+  
+  // block any other thread from starting a concurrent eviction for this bucket
+  // (prevent duplicate buf for the same blockno)
+  acquire(&bcache.eviction_locks[key]);
+
+  // Check again, is the block already cached?
+  // no other allocation targeting this bucket will happen while we are holding eviction_locks[key],
+  // which means this bucket's linked list structure can not change.
+  // so it's ok here to iterate through `bcache.bufmap[key]` without holding
+  // it's cooresponding bucket lock, since we are holding a much stronger eviction_locks[key].
+  for(b = bcache.bufmap[key].next; b; b = b->next){
+    if(b->dev == dev && b->blockno == blockno){
+      acquire(&bcache.bufmap_locks[key]); // must do, for `refcnt++`
+      b->refcnt++;
+      release(&bcache.bufmap_locks[key]);
+      release(&bcache.eviction_locks[key]);
       acquiresleep(&b->lock);
       return b;
     }
   }
-  panic("bget: no buffers");
+
+  // Still not cached.
+  // we are now only holding eviction lock, none of the bucket locks are held by us.
+  // so it's now safe to acquire any bucket's lock without risking circular wait and deadlock.
+
+  // find the one least-recently-used buf among all buckets.
+  // finish with it's corresponding bucket's lock held.
+  struct buf *before_least = 0; 
+  uint holding_bucket = -1;
+  for(int i = 0; i < NBUFMAP_BUCKET; i++){
+    // before acquiring, we are either holding nothing, or only locks of
+    // buckets that are *on the left side* of the current bucket
+    // so no circular wait can ever happen here. (safe from deadlock)
+    acquire(&bcache.bufmap_locks[i]);
+    int newfound = 0; // new least-recently-used buf found in this bucket
+    for(b = &bcache.bufmap[i]; b->next; b = b->next) {
+      if(b->next->refcnt == 0 && (!before_least || b->next->lastuse < before_least->next->lastuse)) {
+        before_least = b;
+        newfound = 1;
+      }
+    }
+    if(!newfound) {
+      release(&bcache.bufmap_locks[i]);
+    } else {
+      if(holding_bucket != -1) release(&bcache.bufmap_locks[holding_bucket]);
+      holding_bucket = i;
+      // keep holding this bucket's lock....
+    }
+  }
+  if(!before_least) {
+    panic("bget: no buffers");
+  }
+  b = before_least->next;
+  
+  if(holding_bucket != key) {
+    // remove the buf from it's original bucket
+    before_least->next = b->next;
+    release(&bcache.bufmap_locks[holding_bucket]);
+    // rehash and add it to the correct bucket
+    acquire(&bcache.bufmap_locks[key]);
+    b->next = bcache.bufmap[key].next;
+    bcache.bufmap[key].next = b;
+  }
+  
+  b->dev = dev;
+  b->blockno = blockno;
+  b->refcnt = 1;
+  b->valid = 0;
+  release(&bcache.bufmap_locks[key]);
+  release(&bcache.eviction_locks[key]);
+  acquiresleep(&b->lock);
+  return b;
 }
 
 // Return a locked buf with the contents of the indicated block.
@@ -108,11 +195,10 @@ bwrite(struct buf *b)
 {
   if(!holdingsleep(&b->lock))
     panic("bwrite");
-  virtio_disk_rw(b, 1);
+	virtio_disk_rw(b, 1);
 }
 
 // Release a locked buffer.
-// Move to the head of the most-recently-used list.
 void
 brelse(struct buf *b)
 {
@@ -121,33 +207,31 @@ brelse(struct buf *b)
 
   releasesleep(&b->lock);
 
-  acquire(&bcache.lock);
+  uint key = BUFMAP_HASH(b->dev, b->blockno);
+
+  acquire(&bcache.bufmap_locks[key]);
   b->refcnt--;
   if (b->refcnt == 0) {
-    // no one is waiting for it.
-    b->next->prev = b->prev;
-    b->prev->next = b->next;
-    b->next = bcache.head.next;
-    b->prev = &bcache.head;
-    bcache.head.next->prev = b;
-    bcache.head.next = b;
+    b->lastuse = ticks;
   }
-  
-  release(&bcache.lock);
+  release(&bcache.bufmap_locks[key]);
 }
 
 void
 bpin(struct buf *b) {
-  acquire(&bcache.lock);
+  uint key = BUFMAP_HASH(b->dev, b->blockno);
+
+  acquire(&bcache.bufmap_locks[key]);
   b->refcnt++;
-  release(&bcache.lock);
+  release(&bcache.bufmap_locks[key]);
 }
 
 void
 bunpin(struct buf *b) {
-  acquire(&bcache.lock);
+  uint key = BUFMAP_HASH(b->dev, b->blockno);
+
+  acquire(&bcache.bufmap_locks[key]);
   b->refcnt--;
-  release(&bcache.lock);
+  release(&bcache.bufmap_locks[key]);
 }
 
-
diff --git a/kernel/buf.h b/kernel/buf.h
index 4616e9e..56560a1 100644
--- a/kernel/buf.h
+++ b/kernel/buf.h
@@ -5,7 +5,7 @@ struct buf {
   uint blockno;
   struct sleeplock lock;
   uint refcnt;
-  struct buf *prev; // LRU cache list
+  uint lastuse;
   struct buf *next;
   uchar data[BSIZE];
 };
diff --git a/kernel/kalloc.c b/kernel/kalloc.c
index fa6a0ac..35b94fb 100644
--- a/kernel/kalloc.c
+++ b/kernel/kalloc.c
@@ -8,6 +8,7 @@
 #include "spinlock.h"
 #include "riscv.h"
 #include "defs.h"
+#include "param.h"
 
 void freerange(void *pa_start, void *pa_end);
 
@@ -21,12 +22,26 @@ struct run {
 struct {
   struct spinlock lock;
   struct run *freelist;
-} kmem;
+} kmem[NCPU];
+
+char *kmem_lock_names[] = {
+  "kmem_cpu_0",
+  "kmem_cpu_1",
+  "kmem_cpu_2",
+  "kmem_cpu_3",
+  "kmem_cpu_4",
+  "kmem_cpu_5",
+  "kmem_cpu_6",
+  "kmem_cpu_7",
+};
 
 void
 kinit()
 {
-  initlock(&kmem.lock, "kmem");
+  for(int i=0;i<NCPU;i++) {
+
+    initlock(&kmem[i].lock, kmem_lock_names[i]);
+  }
   freerange(end, (void*)PHYSTOP);
 }
 
@@ -56,10 +71,16 @@ kfree(void *pa)
 
   r = (struct run*)pa;
 
-  acquire(&kmem.lock);
-  r->next = kmem.freelist;
-  kmem.freelist = r;
-  release(&kmem.lock);
+  push_off();
+
+  int cpu = cpuid();
+
+  acquire(&kmem[cpu].lock);
+  r->next = kmem[cpu].freelist;
+  kmem[cpu].freelist = r;
+  release(&kmem[cpu].lock);
+
+  pop_off();
 }
 
 // Allocate one 4096-byte page of physical memory.
@@ -70,11 +91,36 @@ kalloc(void)
 {
   struct run *r;
 
-  acquire(&kmem.lock);
-  r = kmem.freelist;
+  push_off();
+
+  int cpu = cpuid();
+
+  acquire(&kmem[cpu].lock);
+
+  if(!kmem[cpu].freelist) { // no page left for this cpu
+    int steal_left = 64; // steal 64 pages from other cpu(s)
+    for(int i=0;i<NCPU;i++) {
+      if(i == cpu) continue; // no self-robbery
+      acquire(&kmem[i].lock);
+      struct run *rr = kmem[i].freelist;
+      while(rr && steal_left) {
+        kmem[i].freelist = rr->next;
+        rr->next = kmem[cpu].freelist;
+        kmem[cpu].freelist = rr;
+        rr = kmem[i].freelist;
+        steal_left--;
+      }
+      release(&kmem[i].lock);
+      if(steal_left == 0) break; // done stealing
+    }
+  }
+
+  r = kmem[cpu].freelist;
   if(r)
-    kmem.freelist = r->next;
-  release(&kmem.lock);
+    kmem[cpu].freelist = r->next;
+  release(&kmem[cpu].lock);
+
+  pop_off();
 
   if(r)
     memset((char*)r, 5, PGSIZE); // fill with junk
diff --git a/time.txt b/time.txt
new file mode 100644
index 0000000..f599e28
--- /dev/null
+++ b/time.txt
@@ -0,0 +1 @@
+10
