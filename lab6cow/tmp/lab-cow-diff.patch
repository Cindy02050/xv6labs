diff --git a/kernel/defs.h b/kernel/defs.h
index 4b9bbc0..f99d927 100644
--- a/kernel/defs.h
+++ b/kernel/defs.h
@@ -63,6 +63,8 @@ void            ramdiskrw(struct buf*);
 void*           kalloc(void);
 void            kfree(void *);
 void            kinit(void);
+void            krefpage(void *);
+void            *kcopy_n_deref(void *pa);
 
 // log.c
 void            initlog(int, struct superblock*);
@@ -171,6 +173,8 @@ uint64          walkaddr(pagetable_t, uint64);
 int             copyout(pagetable_t, uint64, char *, uint64);
 int             copyin(pagetable_t, char *, uint64, uint64);
 int             copyinstr(pagetable_t, char *, uint64, uint64);
+int             uvmcheckcowpage(uint64 va);
+int             uvmcowcopy(uint64 va);
 
 // plic.c
 void            plicinit(void);
diff --git a/kernel/kalloc.c b/kernel/kalloc.c
index fa6a0ac..97d0587 100644
--- a/kernel/kalloc.c
+++ b/kernel/kalloc.c
@@ -23,10 +23,25 @@ struct {
   struct run *freelist;
 } kmem;
 
+// For indexing the copy-on-write page reference count array
+#define PA2PGREF_ID(p) (((p)-KERNBASE)/PGSIZE)
+#define PGREF_MAX_ENTRIES PA2PGREF_ID(PHYSTOP)
+
+struct spinlock pgreflock;
+int pageref[PGREF_MAX_ENTRIES]; // reference count for each physical page
+// note:  reference counts are incremented on fork, not on mapping. this means that
+//        multiple mappings of the same physical page within a single process are only
+//        counted as one reference.
+//        this shouldn't be a problem, though. as there's no way for a user program to map
+//        a physical page twice within it's address space in xv6.
+
+#define PA2PGREF(p) pageref[PA2PGREF_ID((uint64)(p))]
+
 void
 kinit()
 {
   initlock(&kmem.lock, "kmem");
+  initlock(&pgreflock, "pgref");
   freerange(end, (void*)PHYSTOP);
 }
 
@@ -51,15 +66,22 @@ kfree(void *pa)
   if(((uint64)pa % PGSIZE) != 0 || (char*)pa < end || (uint64)pa >= PHYSTOP)
     panic("kfree");
 
-  // Fill with junk to catch dangling refs.
-  memset(pa, 1, PGSIZE);
+  acquire(&pgreflock);
+  if(--PA2PGREF(pa) <= 0) {
+    // when the reference count of the page goes to zero, free the page
 
-  r = (struct run*)pa;
+    // Fill with junk to catch dangling refs.
+    // pa will be memset multiple times if race-condition occurred.
+    memset(pa, 1, PGSIZE);
 
-  acquire(&kmem.lock);
-  r->next = kmem.freelist;
-  kmem.freelist = r;
-  release(&kmem.lock);
+    r = (struct run*)pa;
+
+    acquire(&kmem.lock);
+    r->next = kmem.freelist;
+    kmem.freelist = r;
+    release(&kmem.lock);
+  }
+  release(&pgreflock);
 }
 
 // Allocate one 4096-byte page of physical memory.
@@ -76,7 +98,44 @@ kalloc(void)
     kmem.freelist = r->next;
   release(&kmem.lock);
 
-  if(r)
+  if(r){
     memset((char*)r, 5, PGSIZE); // fill with junk
+    // reference count for a physical page is always 1 after allocation.
+    // (no need to lock this operation)
+    PA2PGREF(r) = 1;
+  }
+  
   return (void*)r;
 }
+// Decrease reference to the page by one if it's more than one, then
+// allocate a new physical page and copy the page into it.
+// (Effectively turing one reference into one copy.)
+// 
+// Do nothing and simply return pa when reference count is already
+// less than or equal to 1.
+void *kcopy_n_deref(void *pa) {
+  acquire(&pgreflock);
+
+  if(PA2PGREF(pa) <= 1) {
+    release(&pgreflock);
+    return pa;
+  }
+
+  uint64 newpa = (uint64)kalloc();
+  if(newpa == 0) {
+    release(&pgreflock);
+    return 0; // out of memory
+  }
+  memmove((void*)newpa, (void*)pa, PGSIZE);
+  PA2PGREF(pa)--;
+
+  release(&pgreflock);
+  return (void*)newpa;
+}
+
+// increase reference count of the page by one
+void krefpage(void *pa) {
+  acquire(&pgreflock);
+  PA2PGREF(pa)++;
+  release(&pgreflock);
+}
diff --git a/kernel/riscv.h b/kernel/riscv.h
index 0aec003..e1fef6c 100644
--- a/kernel/riscv.h
+++ b/kernel/riscv.h
@@ -331,7 +331,7 @@ sfence_vma()
 #define PTE_W (1L << 2)
 #define PTE_X (1L << 3)
 #define PTE_U (1L << 4) // 1 -> user can access
-
+#define PTE_COW (1L << 8) // 是否为懒复制页，使用页表项 flags 中保留的第 8 位表示
 // shift a physical address to the right place for a PTE.
 #define PA2PTE(pa) ((((uint64)pa) >> 12) << 10)
 
diff --git a/kernel/trap.c b/kernel/trap.c
index a63249e..28c1978 100644
--- a/kernel/trap.c
+++ b/kernel/trap.c
@@ -67,6 +67,10 @@ usertrap(void)
     syscall();
   } else if((which_dev = devintr()) != 0){
     // ok
+  } else if((r_scause() == 13 || r_scause() == 15) && uvmcheckcowpage(r_stval())) { // copy-on-write
+    if(uvmcowcopy(r_stval()) == -1){
+      p->killed = 1;
+    }
   } else {
     printf("usertrap(): unexpected scause %p pid=%d\n", r_scause(), p->pid);
     printf("            sepc=%p stval=%p\n", r_sepc(), r_stval());
@@ -217,4 +221,3 @@ devintr()
     return 0;
   }
 }
-
diff --git a/kernel/vm.c b/kernel/vm.c
index bccb405..4cb22ff 100644
--- a/kernel/vm.c
+++ b/kernel/vm.c
@@ -5,6 +5,8 @@
 #include "riscv.h"
 #include "defs.h"
 #include "fs.h"
+#include "spinlock.h"
+#include "proc.h"
 
 /*
  * the kernel's page table.
@@ -311,7 +313,7 @@ uvmcopy(pagetable_t old, pagetable_t new, uint64 sz)
   pte_t *pte;
   uint64 pa, i;
   uint flags;
-  char *mem;
+  // char *mem;
 
   for(i = 0; i < sz; i += PGSIZE){
     if((pte = walk(old, i, 0)) == 0)
@@ -319,14 +321,25 @@ uvmcopy(pagetable_t old, pagetable_t new, uint64 sz)
     if((*pte & PTE_V) == 0)
       panic("uvmcopy: page not present");
     pa = PTE2PA(*pte);
+    if(*pte & PTE_W) {
+      // clear out PTE_W for parent, set PTE_COW
+      *pte = (*pte & ~PTE_W) | PTE_COW;
+    }
     flags = PTE_FLAGS(*pte);
-    if((mem = kalloc()) == 0)
-      goto err;
-    memmove(mem, (char*)pa, PGSIZE);
-    if(mappages(new, i, PGSIZE, (uint64)mem, flags) != 0){
-      kfree(mem);
+    // map physical page of parent directly to child (copy-on-write)
+    // since the write flag has already been cleared for the parent
+    // the child mapping won't have the write flag as well.
+    //
+    // for page that is already read-only for parent, it will be read-
+    // only for child as well.
+    // for read-only page that is also a cow page, the PTE_COW flag will
+    // be copied over to child page, making it a cow page automatically.
+    if(mappages(new, i, PGSIZE, (uint64)pa, flags) != 0){
       goto err;
     }
+    // for any cases above, we created a new reference to the physical
+    // page, so increase reference count by one.
+    krefpage((void*)pa);
   }
   return 0;
 
@@ -357,6 +370,8 @@ copyout(pagetable_t pagetable, uint64 dstva, char *src, uint64 len)
   uint64 n, va0, pa0;
 
   while(len > 0){
+    if(uvmcheckcowpage(dstva))
+      uvmcowcopy(dstva);
     va0 = PGROUNDDOWN(dstva);
     pa0 = walkaddr(pagetable, va0);
     if(pa0 == 0)
@@ -440,3 +455,38 @@ copyinstr(pagetable_t pagetable, char *dst, uint64 srcva, uint64 max)
     return -1;
   }
 }
+ 
+// Check if a given virtual address points to a copy-on-write page
+int uvmcheckcowpage(uint64 va) {
+  pte_t *pte;
+  struct proc *p = myproc();
+  
+  return va < p->sz // within size of memory for the process
+    && ((pte = walk(p->pagetable, va, 0))!=0)
+    && (*pte & PTE_V) // page table entry exists
+    && (*pte & PTE_COW); // page is a cow page
+}
+
+// Copy the cow page, then map it as writable
+int uvmcowcopy(uint64 va) {
+  pte_t *pte;
+  struct proc *p = myproc();
+
+  if((pte = walk(p->pagetable, va, 0)) == 0)
+    panic("uvmcowcopy: walk");
+  
+  // copy the cow page
+  // (no copying will take place if reference count is already 1)
+  uint64 pa = PTE2PA(*pte);
+  uint64 new = (uint64)kcopy_n_deref((void*)pa);
+  if(new == 0)
+    return -1;
+  
+  // map as writable, remove the cow flag
+  uint64 flags = (PTE_FLAGS(*pte) | PTE_W) & ~PTE_COW;
+  uvmunmap(p->pagetable, PGROUNDDOWN(va), 1, 0);
+  if(mappages(p->pagetable, va, 1, new, flags) == -1) {
+    panic("uvmcowcopy: mappages");
+  }
+  return 0;
+}
diff --git a/time.txt b/time.txt
new file mode 100644
index 0000000..b8626c4
--- /dev/null
+++ b/time.txt
@@ -0,0 +1 @@
+4
